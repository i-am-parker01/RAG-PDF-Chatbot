{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iveR-MGbPNBi"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install dependencies\n",
        "!pip install gradio groq sentence-transformers faiss-cpu PyPDF2 -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Imports\n",
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import gradio as gr\n",
        "from groq import Groq"
      ],
      "metadata": {
        "id": "idyUOBovQNwy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Configure Groq API\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "l8govTPgRhYo"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: PDF Loading, Chunking, Embeddings, FAISS\n",
        "def pdf_to_text(pdf_path: str) -> str:\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def text_to_chunks(text: str, chunk_size: int = 400, overlap: int = 100) -> List[str]:\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = words[i:i+chunk_size]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_texts(texts: List[str]) -> np.ndarray:\n",
        "    return embed_model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "def build_faiss_index(chunks: List[str]):\n",
        "    embeddings = embed_texts(chunks)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "    return index, embeddings\n"
      ],
      "metadata": {
        "id": "J8On4Oe9S3e0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Retrieval + Groq Query\n",
        "def search_index(query: str, chunks: List[str], index, top_k: int = 3):\n",
        "    query_vec = embed_texts([query])\n",
        "    scores, indices = index.search(query_vec, top_k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "def query_groq(query: str, context_chunks: List[str]):\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    prompt = f\"\"\"\n",
        "You are a super intelligent and a helpful assistant built and engineered by Parker. Use the following context to answer the question.\n",
        "If the answer is not in the context, reply with \"I don't know.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",  # valid Groq model\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.0,\n",
        "        max_tokens=512\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "JvsjFA6PS3l5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: RAG Chatbot Logic\n",
        "chunks, index = None, None  # global vars\n",
        "\n",
        "def ingest_pdf(file):\n",
        "    global chunks, index\n",
        "    raw_text = pdf_to_text(file.name)\n",
        "    chunks = text_to_chunks(raw_text)\n",
        "    index, _ = build_faiss_index(chunks)\n",
        "    return \"PDF process Loading.....\"\n",
        "\n",
        "def chatbot(user_input, history):\n",
        "    if chunks is None or index is None:\n",
        "        return history + [[user_input, \"‚ö†Ô∏è Please upload and process a PDF first.\"]]\n",
        "\n",
        "    retrieved_chunks = search_index(user_input, chunks, index)\n",
        "    answer = query_groq(user_input, retrieved_chunks)\n",
        "    return history + [[user_input, answer]]\n"
      ],
      "metadata": {
        "id": "5x6ASGVwS3og"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Launch Gradio Chatbot UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üìö RAG Chatbot with Groq + FAISS\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pdf_upload = gr.File(label=\"üìÇ Upload your PDF\", file_types=[\".pdf\"])\n",
        "        process_btn = gr.Button(\"Process PDF\")\n",
        "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    chatbot_ui = gr.Chatbot()\n",
        "    user_input = gr.Textbox(label=\"Ask a question\")\n",
        "    clear_btn = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    process_btn.click(ingest_pdf, inputs=pdf_upload, outputs=status)\n",
        "    user_input.submit(chatbot, [user_input, chatbot_ui], chatbot_ui)\n",
        "    clear_btn.click(lambda: None, None, chatbot_ui, queue=False)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "7NBE7eIjS3sC",
        "outputId": "ef37b503-0e5e-40af-964b-f4e2cf2e75bd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1790768962.py:10: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_ui = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://32e59c74c8163ae525.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://32e59c74c8163ae525.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIYxhmEnUeOu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u4zzI1r3WnYI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwyv9DhqWp4s"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F0CyDV2ZU2pl"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}